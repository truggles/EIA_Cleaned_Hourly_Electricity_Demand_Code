{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Which Input Files to Use\n",
    "The default settings will use the input files recently produced in Step 1) using the notebook `get_eia_demand_data.ipynb`. For those interested in reproducing the exact results included in the repository, you will need to point to the files containing the original `raw` EIA demand data that we querried on 10 Sept 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_subregions = True\n",
    "\n",
    "merge_with_step1_files = True # used to run step 2 on the most recent files\n",
    "merge_with_10sept2019_files = False # used to reproduce the documented results\n",
    "assert((merge_with_step1_files != merge_with_10sept2019_files) and \n",
    "       (merge_with_step1_files == True or merge_with_10sept2019_files == True)), \"One of these must be true: 'merge_with_step1_files' and 'merge_with_10sept2019_files'\"\n",
    "\n",
    "if merge_with_step1_files:\n",
    "    input_path = './data' if not include_subregions else './data_subregions'\n",
    "\n",
    "if merge_with_10sept2019_files:\n",
    "    # input_path is the path to the downloaded data from Zenodo: https://zenodo.org/record/3517197\n",
    "    input_path = '/BASE/PATH/TO/ZENODO'\n",
    "    input_path += '/data/release_2019_Oct/original_eia_files'\n",
    "    assert(os.path.exists(input_path)), f\"You must set the base directory for the Zenodo data {input_path} does not exist\"\n",
    "    # If you did not run step 1, make the /data directory\n",
    "    if not os.path.exists('./data'):\n",
    "        os.mkdir('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directories\n",
    "out_base = './data_subregions/final_results'\n",
    "if not os.path.exists(out_base):\n",
    "    os.mkdir(out_base)\n",
    "    for subdir in ['balancing_authorities', 'regions', 'interconnects', 'contiguous_US']:\n",
    "        os.mkdir(f\"{out_base}/{subdir}\")\n",
    "        print(f\"Final results files will be located here: {out_base}/{subdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 56 balancing authorities that have demand (BA)\n",
    "def return_all_regions():\n",
    "    return [\n",
    "                'AEC', 'AECI', 'CPLE', 'CPLW',\n",
    "                'DUK', 'FMPP', 'FPC',\n",
    "                'FPL', 'GVL', 'HST', 'ISNE',\n",
    "                'JEA', 'LGEE', 'MISO', 'NSB',\n",
    "                'NYIS', 'PJM', 'SC',\n",
    "                'SCEG', 'SOCO',\n",
    "                'SPA', 'SWPP', 'TAL', 'TEC',\n",
    "                'TVA', 'ERCO',\n",
    "                'AVA', 'AZPS', 'BANC', 'BPAT',\n",
    "                'CHPD', 'CISO', 'DOPD',\n",
    "                'EPE', 'GCPD', 'IID',\n",
    "                'IPCO', 'LDWP', 'NEVP', 'NWMT',\n",
    "                'PACE', 'PACW', 'PGE', 'PNM',\n",
    "                'PSCO', 'PSEI', 'SCL', 'SRP',\n",
    "                'TEPC', 'TIDC', 'TPWR', 'WACM',\n",
    "                'WALC', 'WAUW',\n",
    "                'OVEC', 'SEC',\n",
    "                ]\n",
    "\n",
    "# All 54 \"usable\" balancing authorities (BA) (excludes OVEC, SEC, and NSB)\n",
    "# These 3 have significant\n",
    "# enough reporting problems that we do not impute cleaned data for them.\n",
    "def return_usable_BAs(incl_subregs):\n",
    "    # Return all regions except OVEC, SEC, and NSB\n",
    "    regions = [reg for reg in return_all_regions() if reg not in ['OVEC', 'SEC', 'NSB']]\n",
    "    if incl_subregs:\n",
    "        regions = append_sub_regions(regions)\n",
    "        regions = remove_regions(regions)\n",
    "    return regions\n",
    "    \n",
    "\n",
    "# append subregions to regions list\n",
    "def append_sub_regions(l):\n",
    "    subregions = [\n",
    "        'CISO-PGAE', 'CISO-SCE', 'CISO-SDGE', 'CISO-VEA',\n",
    "\n",
    "        'ERCO-COAS', 'ERCO-EAST', 'ERCO-FWES', 'ERCO-NCEN',\n",
    "        'ERCO-NRTH', 'ERCO-SCEN', 'ERCO-SOUT', 'ERCO-WEST',\n",
    "\n",
    "        'ISNE-4001', 'ISNE-4002', 'ISNE-4003', 'ISNE-4004',\n",
    "        'ISNE-4005', 'ISNE-4006', 'ISNE-4007', 'ISNE-4008',\n",
    "\n",
    "        'MISO-0001', 'MISO-0004', 'MISO-0006', 'MISO-0027', 'MISO-0035', 'MISO-8910',\n",
    "\n",
    "        'NYIS-ZONA', 'NYIS-ZONB', 'NYIS-ZONC', 'NYIS-ZOND', 'NYIS-ZONE',\n",
    "        'NYIS-ZONF', 'NYIS-ZONG', 'NYIS-ZONH', 'NYIS-ZONI', 'NYIS-ZONJ', 'NYIS-ZONK',\n",
    "\n",
    "        'PJM-AE', 'PJM-AEP', 'PJM-AP', 'PJM-ATSI', 'PJM-BC', 'PJM-CE', 'PJM-DAY',\n",
    "        'PJM-DEOK', 'PJM-DOM', 'PJM-DPL', 'PJM-DUQ', 'PJM-EKPC', 'PJM-JC',\n",
    "        'PJM-ME', 'PJM-PE', 'PJM-PEP', 'PJM-PL', 'PJM-PN', 'PJM-PS', 'PJM-RECO',\n",
    "\n",
    "        'PNM-KAFB', 'PNM-KCEC', 'PNM-LAC', 'PNM-NTUA', 'PNM-PNM', 'PNM-TSGT',\n",
    "\n",
    "        'SWPP-CSWS', 'SWPP-EDE', 'SWPP-GRDA', 'SWPP-INDN', 'SWPP-KACY',\n",
    "        'SWPP-KCPL', 'SWPP-LES', 'SWPP-MPS', 'SWPP-NPPD', 'SWPP-OKGE', 'SWPP-OPPD',\n",
    "        'SWPP-SECI', 'SWPP-SPRM', 'SWPP-SPS', 'SWPP-WAUE', 'SWPP-WFEC', 'SWPP-WR',    \n",
    "    ]\n",
    "    for subR in subregions:\n",
    "        l.append(subR)\n",
    "    return l\n",
    "\n",
    "# Removes BAs with subregions to avoid double counting in the imputation step.\n",
    "# This excludes the ERCO subregions because of their 2019 data start date.\n",
    "def remove_regions(l):\n",
    "    \n",
    "    to_exclude = [\n",
    "        \n",
    "        # because subregions are so small there are too many\n",
    "        # identical run filters that result in many anomalous regions\n",
    "        'PNM-KAFB', 'PNM-KCEC', 'PNM-LAC', 'PNM-NTUA', 'PNM-PNM', 'PNM-TSGT',\n",
    "        \n",
    "        # This PJM region has last data entry April 2019\n",
    "        'PJM-RECO',\n",
    "              \n",
    "        # This BA has last data entry January 2020\n",
    "        'NSB',\n",
    "        \n",
    "        # BAs with subregions\n",
    "        'CISO', 'ERCO', 'ISNE', 'MISO', 'NYIS', 'PJM', 'SWPP',\n",
    "    ]\n",
    "    \n",
    "    for rm in to_exclude:\n",
    "        if rm in l:\n",
    "            l.remove(rm)\n",
    "    \n",
    "    return l\n",
    "\n",
    "# mapping of each balancing authority (BA) to its associated\n",
    "# U.S. interconnect (IC).\n",
    "def return_ICs_from_BAs():\n",
    "    return {\n",
    "        'EASTERN_IC' : [\n",
    "                'AEC', 'AECI', 'CPLE', 'CPLW',\n",
    "                'DUK', 'FMPP', 'FPC',\n",
    "                'FPL', 'GVL', 'HST', 'ISNE',\n",
    "                'JEA', 'LGEE', 'MISO', 'NSB',\n",
    "                'NYIS', 'PJM', 'SC',\n",
    "                'SCEG', 'SOCO',\n",
    "                'SPA', 'SWPP', 'TAL', 'TEC',\n",
    "                'TVA',\n",
    "                'OVEC', 'SEC',\n",
    "                ],\n",
    "        'TEXAS_IC' : [\n",
    "                'ERCO',\n",
    "                ],\n",
    "        'WESTERN_IC' : [\n",
    "                'AVA', 'AZPS', 'BANC', 'BPAT',\n",
    "                'CHPD', 'CISO', 'DOPD',\n",
    "                'EPE', 'GCPD',\n",
    "                'IID',\n",
    "                'IPCO', 'LDWP', 'NEVP', 'NWMT',\n",
    "                'PACE', 'PACW', 'PGE', 'PNM',\n",
    "                'PSCO', 'PSEI', 'SCL', 'SRP',\n",
    "                'TEPC', 'TIDC', 'TPWR', 'WACM',\n",
    "                'WALC', 'WAUW',\n",
    "                ]\n",
    "        }\n",
    "\n",
    "# Defines a mapping between the balancing authorities (BAs)\n",
    "# and their locally defined region based on EIA naming.\n",
    "# This uses a json file defining the mapping.\n",
    "def return_BAs_per_region_map():\n",
    "\n",
    "    regions = {\n",
    "            'CENT' : 'Central',\n",
    "            'MIDW' : 'Midwest',\n",
    "            'TEN' : 'Tennessee',\n",
    "            'SE' : 'Southeast',\n",
    "            'FLA' : 'Florida',\n",
    "            'CAR' : 'Carolinas',\n",
    "            'MIDA' : 'Mid-Atlantic',\n",
    "            'NY' : 'New York',\n",
    "            'NE' : 'New England',\n",
    "            'TEX' : 'Texas',\n",
    "            'CAL' : 'California',\n",
    "            'NW' : 'Northwest',\n",
    "            'SW' : 'Southwest'\n",
    "    }\n",
    "\n",
    "    rtn_map = {}\n",
    "    for k, v in regions.items():\n",
    "        rtn_map[k] = []\n",
    "\n",
    "    # Load EIA's Blancing Authority Acronym table\n",
    "    # https://www.eia.gov/realtime_grid/\n",
    "    df = pd.read_csv('data/balancing_authority_acronyms.csv',\n",
    "            skiprows=1) # skip first row as it is source info\n",
    "\n",
    "    # Loop over all rows and fill map\n",
    "    for idx in df.index:\n",
    "\n",
    "        # Skip Canada and Mexico\n",
    "        if df.loc[idx, 'Region'] in ['Canada', 'Mexico']:\n",
    "            continue\n",
    "\n",
    "        reg_acronym = ''\n",
    "        # Get region to acronym\n",
    "        for k, v in regions.items():\n",
    "            if v == df.loc[idx, 'Region']:\n",
    "                reg_acronym = k\n",
    "                break\n",
    "        assert(reg_acronym != '')\n",
    "\n",
    "        rtn_map[reg_acronym].append(df.loc[idx, 'Code'])\n",
    "\n",
    "    tot = 0\n",
    "    for k, v in rtn_map.items():\n",
    "        tot += len(v)\n",
    "    print(f\"Total US48 BAs mapped {tot}.  Recall 11 are generation only.\")\n",
    "\n",
    "    return rtn_map\n",
    "\n",
    "\n",
    "# Assume the MICE results file is a subset of the original hours\n",
    "def trim_rows_to_match_length(mice, df):\n",
    "    mice_start = mice.loc[0, 'date_time']\n",
    "    mice_end = mice.loc[len(mice.index)-1, 'date_time']\n",
    "    to_drop = []\n",
    "    for idx in df.index:\n",
    "        if pd.to_datetime(df.loc[idx, 'date_time']) != mice_start:\n",
    "            to_drop.append(idx)\n",
    "        else: # stop once equal\n",
    "            break\n",
    "    for idx in reversed(df.index):\n",
    "        if pd.to_datetime(df.loc[idx, 'date_time']) != mice_end:\n",
    "            to_drop.append(idx)\n",
    "        else: # stop once equal\n",
    "            break\n",
    "    \n",
    "    df = df.drop(to_drop, axis=0)\n",
    "    df = df.reset_index()\n",
    "    assert(len(mice.index) == len(df.index))\n",
    "    return df\n",
    "\n",
    "# Remove imputed values and set to 0 when region stopped reporting\n",
    "# These region are often absorbed by others, so keeping them would double count\n",
    "def remove_values_stopped_reporting(ba, mice, df):\n",
    "    # Create a mask for rows with 'MISSING' or 'EMPTY' in 'demand (MW)'\n",
    "    missing_mask = (df['demand (MW)'] == 'MISSING') | (df['demand (MW)'] == 'EMPTY')\n",
    "    # Find where missing values are followed only by missing values until the end\n",
    "    cumulative_valid = missing_mask[::-1].cummin()[::-1]  # Check if all subsequent values are missing/empty\n",
    "    # Get the first index where it's True\n",
    "    first_all_missing_index = cumulative_valid.idxmax() if cumulative_valid.any() else None\n",
    "    if first_all_missing_index is not None:\n",
    "        # Set all values in MICE after the last known value to 0\n",
    "        mice.loc[first_all_missing_index:, ba.replace('-', '.')] = 0\n",
    "\n",
    "    # Similarly, if the series starts with missing values, set these to 0\n",
    "    # Find the first index where the value is NOT 'MISSING' or 'EMPTY'\n",
    "    first_non_missing_index = df[~df['demand (MW)'].isin(['MISSING', 'EMPTY'])].index.min()\n",
    "    # If there are leading 'MISSING' values, set them to 0\n",
    "    if first_non_missing_index > 0:\n",
    "        # Set all values in MICE before the first non-missing value to 0\n",
    "        mice.loc[:first_non_missing_index, ba.replace('-', '.')] = 0\n",
    "    \n",
    "    return mice\n",
    "\n",
    "\n",
    "\n",
    "# Load balancing authority files already containing the full MICE results.\n",
    "# Aggregate associated regions into regional, interconnect, or CONUS files.\n",
    "# Treat 'MISSING' and 'EMPTY' values as zeros when aggregating.\n",
    "def merge_BAs(region, bas, out_base, folder):\n",
    "    \n",
    "    print(region, bas)\n",
    "    \n",
    "    # Remove BAs which are generation only as well as SEC, OVEC, and NSB.\n",
    "    # See main README regarding SEC and OVEC.\n",
    "    usable_BAs = return_usable_BAs(incl_subregs=include_subregions)\n",
    "    good_bas = []\n",
    "    for ba in bas:\n",
    "        ba_or_subs = [ba_or_sub for ba_or_sub in usable_BAs if ba == ba_or_sub.split('-')[0] or ba == ba_or_sub]\n",
    "        # Append BA if no subregions otherwise append subregions\n",
    "        if len(ba_or_subs)==1:\n",
    "            good_bas.append(ba)\n",
    "        else:\n",
    "            good_bas.extend(ba_or_subs)\n",
    "\n",
    "    # Add up demand for all BAs or subregions when available\n",
    "    master = None\n",
    "    for ba in good_bas:\n",
    "        df = pd.read_csv(f'{out_base}/balancing_authorities/{ba}.csv', na_values=['MISSING', 'EMPTY'])\n",
    "        df = df.fillna(0)\n",
    "        df = df.drop(['category'], axis=1)\n",
    "        \n",
    "        if master is None:\n",
    "            master = df.copy()\n",
    "        else:\n",
    "            master['raw demand (MW)'] += df['raw demand (MW)']\n",
    "            master['cleaned demand (MW)'] += df['cleaned demand (MW)']\n",
    "\n",
    "    # Round demand to integers as input data is integers\n",
    "    master['raw demand (MW)'] = master['raw demand (MW)'].astype(int)\n",
    "    master['cleaned demand (MW)'] = master['cleaned demand (MW)'].astype(int)\n",
    "    \n",
    "    master.to_csv(f'{out_base}/{folder}/{region}.csv', index=False)\n",
    "\n",
    "def ensure_time_component(dt_str):\n",
    "    try:\n",
    "        # Try to parse the datetime string\n",
    "        dt = datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S')\n",
    "    except ValueError:\n",
    "        # If parsing fails, add '00:00:00' to the date string\n",
    "        dt_str = dt_str + ' 00:00:00'\n",
    "        dt = datetime.strptime(dt_str, '%Y-%m-%d %H:%M:%S')\n",
    "    return dt\n",
    "\n",
    "# Do both the distribution of balancing authority level results to new BA files\n",
    "# and generate regional, interconnect, and CONUS aggregate files.\n",
    "def distribute_MICE_results(raw_demand_file_loc, screening_file, mice_results_csv, out_base):\n",
    "\n",
    "    # Load screening results\n",
    "    screening = pd.read_csv(screening_file)\n",
    "    screening['date_time'] = screening['date_time'].apply(ensure_time_component)\n",
    "    # Load MICE results\n",
    "    mice = pd.read_csv(mice_results_csv)\n",
    "    mice['date_time'] = mice['date_time'].apply(ensure_time_component)\n",
    "\n",
    "    screening = trim_rows_to_match_length(mice, screening)\n",
    "    \n",
    "    # Distribute to single BA results files first\n",
    "    print(\"Distribute MICE results per-balancing authority:\")\n",
    "    for ba in return_usable_BAs(incl_subregs=include_subregions):\n",
    "        print(ba)\n",
    "\n",
    "        # Skip balancing authorities where subregion data is available\n",
    "        if os.path.exists(f\"{raw_demand_file_loc}/{ba}-*.csv\"):\n",
    "            print(f\"Skipping {ba} as subregion data is available.\")\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(f\"{raw_demand_file_loc}/{ba}.csv\")\n",
    "        df = trim_rows_to_match_length(mice, df)\n",
    "        mice = remove_values_stopped_reporting(ba, mice, df)\n",
    "    \n",
    "        df_out = pd.DataFrame({\n",
    "            'date_time': df['date_time'],\n",
    "            'raw demand (MW)': df['demand (MW)'],\n",
    "            'category': screening[f'{ba}_category'],\n",
    "            'cleaned demand (MW)': mice[ba.replace('-', '.')],\n",
    "        })\n",
    "        \n",
    "        \n",
    "        df_out.to_csv(f'./{out_base}/balancing_authorities/{ba}.csv', index=False)\n",
    "\n",
    "    # Aggregate balancing authority level results into EIA regions\n",
    "    print(\"\\nEIA regional aggregation:\")\n",
    "    for region, bas in return_BAs_per_region_map().items():\n",
    "        merge_BAs(region, bas, out_base, 'regions')\n",
    "    \n",
    "    # Aggregate balancing authority level results into CONUS interconnects\n",
    "    print(\"\\nCONUS interconnect aggregation:\")\n",
    "    for region, bas in return_ICs_from_BAs().items():\n",
    "        merge_BAs(region, bas, out_base, 'interconnects')\n",
    "    \n",
    "    # Aggregate balancing authority level results into CONUS total\n",
    "    print(\"\\nCONUS total aggregation:\")\n",
    "    merge_BAs('CONUS', return_usable_BAs(incl_subregs=include_subregions), out_base, 'contiguous_US')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the distribution and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output file generated by Step 2 listing the categories for each time step\n",
    "screening_file = os.path.join(input_path, 'csv_MASTER.csv')\n",
    "# The output file generated by Step 3 which runs the MICE algo and has the cleaned demand values\n",
    "mice_file = 'MICE_output/mean_impute_csv_MASTER.csv'\n",
    "\n",
    "\n",
    "distribute_MICE_results(input_path, screening_file, mice_file, out_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test distribution and aggregation\n",
    "This cell simply checks that the results all add up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each value in the vectors\n",
    "def compare(vect1, vect2):\n",
    "    cnt = 0\n",
    "    clean = True\n",
    "    for v1, v2 in zip(vect1, vect2):\n",
    "        if round(v1,4) != round(v2,4):\n",
    "            print(f\"Error at idx {cnt} {v1} != {v2}\")\n",
    "            clean = False\n",
    "        cnt += 1\n",
    "    return clean\n",
    "    \n",
    "\n",
    "def test_aggregation(raw_demand_file_loc, mice_results_csv, out_base):\n",
    "\n",
    "    # Load MICE results\n",
    "    usable_BAs = return_usable_BAs(incl_subregs=include_subregions)\n",
    "    mice = pd.read_csv(mice_results_csv)\n",
    "    mice['date_time'] = mice['date_time'].apply(ensure_time_component)\n",
    "\n",
    "    # Sum all result BAs\n",
    "    tot_imp = np.zeros(len(mice.index))\n",
    "    for col in mice.columns:\n",
    "        if col.replace('.','-') not in usable_BAs:\n",
    "            continue\n",
    "        mice = remove_values_stopped_reporting(col.replace('.', '-'), mice, pd.read_csv(f\"{raw_demand_file_loc}/{col.replace('.', '-')}.csv\"))\n",
    "        tot_imp += mice[col]\n",
    "\n",
    "    # Sum Raw\n",
    "    tot_raw = np.zeros(len(mice.index))\n",
    "    for ba in return_usable_BAs(incl_subregs=include_subregions):\n",
    "        df = pd.read_csv(f\"{raw_demand_file_loc}/{ba}.csv\", na_values=['MISSING', 'EMPTY'])\n",
    "        df['date_time'] = df['date_time'].apply(ensure_time_component)\n",
    "        df = trim_rows_to_match_length(mice, df)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        tot_raw += df['demand (MW)']\n",
    "    \n",
    "    # Check BA results distribution\n",
    "    print(\"\\nBA Distribution:\")\n",
    "    new_tot_raw = np.zeros(len(mice.index))\n",
    "    new_tot_clean = np.zeros(len(mice.index))\n",
    "    for ba in return_usable_BAs(incl_subregs=include_subregions):\n",
    "        df = pd.read_csv(f\"{out_base}/balancing_authorities/{ba}.csv\", na_values=['MISSING', 'EMPTY'])\n",
    "        df = df.fillna(0)\n",
    "        new_tot_raw += df['raw demand (MW)']\n",
    "        new_tot_clean += df['cleaned demand (MW)']\n",
    "    \n",
    "    assert(compare(tot_raw, new_tot_raw)), \"Error in raw sums.\"\n",
    "    assert(compare(tot_imp, new_tot_clean)), \"Error in imputed values.\"\n",
    "    print(\"BA Distribution okay!\")\n",
    "    \n",
    "    \n",
    "    # Check aggregate balancing authority level results into EIA regions\n",
    "    print(\"\\nEIA regional aggregation:\")\n",
    "    new_tot_raw = np.zeros(len(mice.index))\n",
    "    new_tot_clean = np.zeros(len(mice.index))\n",
    "    for region, bas in return_BAs_per_region_map().items():\n",
    "        df = pd.read_csv(f\"{out_base}/regions/{region}.csv\")\n",
    "        new_tot_raw += df['raw demand (MW)']\n",
    "        new_tot_clean += df['cleaned demand (MW)']\n",
    "    \n",
    "    assert(compare(tot_raw, new_tot_raw)), \"Error in raw sums.\"\n",
    "    assert(compare(tot_imp, new_tot_clean)), \"Error in imputed values.\"\n",
    "    print(\"Regional sums okay!\")\n",
    "    \n",
    "    # Aggregate balancing authority level results into CONUS interconnects\n",
    "    print(\"\\nCONUS interconnect aggregation:\")\n",
    "    new_tot_raw = np.zeros(len(mice.index))\n",
    "    new_tot_clean = np.zeros(len(mice.index))\n",
    "    for region, bas in return_ICs_from_BAs().items():\n",
    "        df = pd.read_csv(f\"{out_base}/interconnects/{region}.csv\")\n",
    "        new_tot_raw += df['raw demand (MW)']\n",
    "        new_tot_clean += df['cleaned demand (MW)']\n",
    "    \n",
    "    assert(compare(tot_raw, new_tot_raw)), \"Error in raw sums.\"\n",
    "    assert(compare(tot_imp, new_tot_clean)), \"Error in imputed values.\"\n",
    "    print(\"Interconnect sums okay!\")\n",
    "           \n",
    "           \n",
    "    # Aggregate balancing authority level results into CONUS total\n",
    "    print(\"\\nCONUS total aggregation:\")\n",
    "    new_tot_raw = np.zeros(len(mice.index))\n",
    "    new_tot_clean = np.zeros(len(mice.index))\n",
    "    df = pd.read_csv(f\"{out_base}/contiguous_US/CONUS.csv\")\n",
    "    new_tot_raw += df['raw demand (MW)']\n",
    "    new_tot_clean += df['cleaned demand (MW)']\n",
    "    \n",
    "    assert(compare(tot_raw, new_tot_raw)), \"Error in raw sums.\"\n",
    "    assert(compare(tot_imp, new_tot_clean)), \"Error in imputed values.\"\n",
    "    print(\"CONUS sums okay!\")\n",
    "\n",
    "\n",
    "test_aggregation(input_path, mice_file, out_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_cleaning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
